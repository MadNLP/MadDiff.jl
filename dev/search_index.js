var documenterSearchIndex = {"docs":
[{"location":"#MadDiff.jl","page":"Home","title":"MadDiff.jl","text":"","category":"section"},{"location":"#MadDiff.ForwardResult","page":"Home","title":"MadDiff.ForwardResult","text":"ForwardResult{VT}\n\nFields\n\ndx::VT: Primal variable sensitivities\ndλ::VT: Constraint dual sensitivities\ndzl::VT: Lower bound dual sensitivities\ndzu::VT: Upper bound dual sensitivities\n\n\n\n\n\n","category":"type"},{"location":"#MadDiff.MadDiffConfig","page":"Home","title":"MadDiff.MadDiffConfig","text":"MadDiffConfig(; kwargs...)\n\nConfiguration options for MadDiffSolver.\n\nKeyword Arguments\n\nkkt_system::Type: KKT system type for sensitivity analysis. Defaults to re-using the solver's KKT system.\nkkt_options::Dict: Options passed to KKT system constructor.\nlinear_solver::Type: Custom linear solver type for KKT system.\nlinear_solver_options::Dict: Options passed to linear solver constructor.\nreuse_kkt::Bool: Whether to reuse the solver's KKT system (default: true). Ignored if kkt_system, kkt_options,                     linear_solver, or linear_solver_options are provided.\nregularization::Symbol: Regularization strategy (:solver, :none, or :inertia. default: :solver).\ninertia_shift_step::Float64: Step size for inertia-based regularization (default: 1.0e-6).\ninertia_max_corrections::Int: Maximum number of inertia correction attempts (default: 50).\nshould_warn_condensed::Bool: Whether to warn when using condensed KKT systems (default: false).\n\n\n\n\n\n","category":"type"},{"location":"#MadDiff.MadDiffSolver","page":"Home","title":"MadDiff.MadDiffSolver","text":"MadDiffSolver(solver; config=MadDiffConfig(), param_pullback=nothing, n_p=0)\n\nCreate a sensitivity solver from a solved MadNLP solver.\n\nArguments\n\nsolver: A solved MadNLP.AbstractMadNLPSolver\nconfig: Optional MadDiffConfig controlling KKT reuse and regularization.           If reuse_kkt=true and no custom KKT options are provided, we           reuse the solver's KKT system; otherwise we build a new one.\nparam_pullback: Optional callback to compute parameter gradients in reverse mode.              Signature: param_pullback(out, adj_x, adj_λ, adj_zl, adj_zu, sens) -> out.              The callback should fill out with ∂L/∂p for each parameter.\nn_p: Number of parameters (required if param_pullback is provided).             Used to pre-allocate the gradient buffer.\n\nExample\n\nsolver = MadNLPSolver(nlp)\n\n# Forward mode:\nsens = MadDiffSolver(solver)\nfwd_result = forward_differentiate!(sens; Dxp_L, Dp_g)\n\n# Reverse mode:\nsens = MadDiffSolver(solver; param_pullback, n_p)\nrev_result = reverse_differentiate!(sens; dL_dx)\n\n\n\n\n\n","category":"type"},{"location":"#MadDiff.ReverseResult","page":"Home","title":"MadDiff.ReverseResult","text":"ReverseResult{VT, GT}\n\nFields\n\ndx::VT: Primal sensitivity vector\ndλ::VT: Dual sensitivity vector\ndzl::VT: Lower bound dual sensitivity\ndzu::VT: Upper bound dual sensitivity\ngrad_p::GT: Parameter gradient vector (if param_pullback callback was set on solver, nothing otherwise)\n\n\n\n\n\n","category":"type"},{"location":"#MadDiff.forward_differentiate!-Tuple{MadDiff.ForwardResult, MadDiffSolver}","page":"Home","title":"MadDiff.forward_differentiate!","text":"forward_differentiate!(result::ForwardResult, sens::MadDiffSolver; kwargs...) -> ForwardResult\n\nCompute forward sensitivities (JVP) for a parameter perturbation, writing to pre-allocated result.\n\nKeyword Arguments\n\nd2L_dxdp: Parameter-Lagrangian cross derivative times Δp: (∂²L/∂x∂p) * Δp (length = n_x)\ndg_dp: Parameter-constraint LHS Jacobian times Δp: (∂g/∂p) * Δp (length = n_con)\ndl_dp: Variable lower bound perturbation times Δp: (∂l/∂p) * Δp (length = n_x)\ndu_dp: Variable upper bound perturbation times Δp: (∂u/∂p) * Δp (length = n_x)\ndlcon_dp: Constraint lower bound perturbation times Δp: (∂lcon/∂p) * Δp (length = n_con)\nducon_dp: Constraint upper bound perturbation times Δp: (∂ucon/∂p) * Δp (length = n_con)\n\nNotes\n\nFor equality constraints (lcon[i] == ucon[i]), provide the same perturbation for both dlcondp[i] and ducondp[i]. The implementation uses (dlcondp + ducondp)/2 for equality constraints. For entries where lcon or ucon is ±Inf, the corresponding dlcondp/ducondp value is ignored. For fixed variables (lvar[i] == uvar[i]) with MakeParameter, the sensitivity dx[i] is set to dl_dp[i].\n\nReturns\n\nThe same result object, with updated values\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.forward_differentiate!-Tuple{MadDiffSolver}","page":"Home","title":"MadDiff.forward_differentiate!","text":"forward_differentiate!(sens::MadDiffSolver; kwargs...) -> ForwardResult\n\nCompute forward sensitivities (JVP) for a parameter perturbation.\n\nAllocates a new ForwardResult. For batch processing, use the pre-allocated variant forward_differentiate!(result, sens; ...) to avoid allocations.\n\nKeyword Arguments\n\nd2L_dxdp: Parameter-Lagrangian cross derivative times Δp: (∂²L/∂x∂p) * Δp (length = n_x)\ndg_dp: Parameter-constraint LHS Jacobian times Δp: (∂g/∂p) * Δp (length = n_con)\ndl_dp: Variable lower bound perturbation times Δp: (∂l/∂p) * Δp (length = n_x)\ndu_dp: Variable upper bound perturbation times Δp: (∂u/∂p) * Δp (length = n_x)\ndlcon_dp: Constraint lower bound perturbation times Δp: (∂lcon/∂p) * Δp (length = n_con)\nducon_dp: Constraint upper bound perturbation times Δp: (∂ucon/∂p) * Δp (length = n_con)\n\nNotes\n\nFor equality constraints (lcon[i] == ucon[i]), provide the same perturbation for both dlcondp[i] and ducondp[i]. The implementation uses (dlcondp + ducondp)/2 for equality constraints. For entries where lcon or ucon is ±Inf, the corresponding dlcondp/ducondp value is ignored. To obtain a full Jacobian w.r.t. parameters, call this once per parameter direction.\n\nReturns\n\nForwardResult containing dx, dλ, dzl, dzu sensitivity vectors\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.forward_differentiate!-Tuple{MadNLP.AbstractMadNLPSolver}","page":"Home","title":"MadDiff.forward_differentiate!","text":"forward_differentiate!(solver; kwargs...) -> ForwardResult\n\nConvenience function for one-shot forward sensitivity computation. For multiple perturbations, use MadDiffSolver.\n\nArguments\n\nsolver: A solved MadNLP.AbstractMadNLPSolver\nkwargs...: Passed to forward_differentiate! and MadDiffConfig\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.make_param_pullback-Tuple{}","page":"Home","title":"MadDiff.make_param_pullback","text":"make_param_pullback(; d2L_dxdp=nothing, dg_dp=nothing, dlcon_dp=nothing, ducon_dp=nothing, dl_dp=nothing, du_dp=nothing)\n\nCreate a param_pullback callback from parameter derivative matrices.\n\nThe returned callback computes grad_p = ∂L/∂p using:\n\ngrad_p = -d2L_dxdp' * dx - dg_dp' * dλ + dlcon_dp' * dλ - dl_dp' * dzl + du_dp' * dzu\n\nArguments\n\nd2L_dxdp: ∂²L/∂x∂p - cross derivative of Lagrangian w.r.t. x and p (nx × np)\ndg_dp: ∂g/∂p - derivative of constraint function w.r.t. p (ncon × np)\ndlcon_dp: ∂lcon/∂p - derivative of constraint lower bounds (ncon × np)\nducon_dp: ∂ucon/∂p - derivative of constraint upper bounds (ncon × np)\ndl_dp: ∂l/∂p - derivative of variable lower bounds (nx × np)\ndu_dp: ∂u/∂p - derivative of variable upper bounds (nx × np)\n\nEquality Constraint Handling\n\nFor equality constraints (lcon == ucon), the contributions from dlcon_dp and ducon_dp are scaled by 0.5 to handle AD through problem construction naturally giving equal values for both.\n\nExample\n\n# Parameter p affects constraint RHS: g(x) >= p\nparam_pullback = make_param_pullback(dlcon_dp=dlcon_dp)\nsens = MadDiffSolver(solver; param_pullback, n_p)\nresult = reverse_differentiate!(sens; dL_dx)\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.reset_sensitivity_cache!-Tuple{MadDiffSolver}","page":"Home","title":"MadDiff.reset_sensitivity_cache!","text":"reset_sensitivity_cache!(sens::MadDiffSolver) -> MadDiffSolver\n\nClear cached forward/reverse work buffers and refresh the sensitivity KKT factorization. Use after re-solving or modifying solver so that sensitivities reflect the latest solution/problem.\n\nIf not called, sensitivities will be incorrect!\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.reverse_differentiate!-Tuple{MadDiff.ReverseResult, MadDiffSolver}","page":"Home","title":"MadDiff.reverse_differentiate!","text":"reverse_differentiate!(result::ReverseResult, sens::MadDiffSolver; kwargs...) -> ReverseResult\n\nCompute reverse sensitivities (VJP) given loss gradients, writing to pre-allocated result.\n\nKeyword Arguments\n\ndL_dx: Gradient of loss with respect to primal variables (∂L/∂x)\ndL_dλ: Gradient of loss with respect to constraint duals (∂L/∂λ)\ndL_dzl: Gradient of loss with respect to lower bound duals (∂L/∂zl)\ndL_dzu: Gradient of loss with respect to upper bound duals (∂L/∂zu)\n\nReturns\n\nThe same result object, with updated values\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.reverse_differentiate!-Tuple{MadDiffSolver}","page":"Home","title":"MadDiff.reverse_differentiate!","text":"reverse_differentiate!(sens::MadDiffSolver; dL_dx=nothing, dL_dλ=nothing, dL_dzl=nothing, dL_dzu=nothing) -> ReverseResult\n\nCompute reverse sensitivities (VJP) given loss gradients.\n\nAllocates a new ReverseResult. For batch processing, use the pre-allocated variant reverse_differentiate!(result, sens; ...) to avoid allocations.\n\nArguments\n\ndL_dx: Gradient of loss with respect to primal variables (∂L/∂x)\ndL_dλ: Gradient of loss with respect to constraint duals (∂L/∂λ)\ndL_dzl: Gradient of loss with respect to lower bound duals (∂L/∂zl)\ndL_dzu: Gradient of loss with respect to upper bound duals (∂L/∂zu)\n\nReturns\n\nReverseResult containing:\ndx: Primal sensitivity vector\ndλ: Constraint dual sensitivity vector\ndzl: Lower bound dual sensitivity vector\ndzu: Upper bound dual sensitivity vector\ngrad_p: Parameter gradient vector (if param_pullback was set; nothing otherwise)\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.reverse_differentiate!-Tuple{MadNLP.AbstractMadNLPSolver}","page":"Home","title":"MadDiff.reverse_differentiate!","text":"reverse_differentiate!(solver; dL_dx=nothing, dL_dλ=nothing, dL_dzl=nothing, dL_dzu=nothing, kwargs...) -> ReverseResult\n\nConvenience function for one-shot reverse sensitivity computation. For multiple gradient computations, use the MadDiffSolver API.\n\n\n\n\n\n","category":"method"}]
}
