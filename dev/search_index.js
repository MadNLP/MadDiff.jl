var documenterSearchIndex = {"docs":
[{"location":"#MadDiff.jl","page":"Home","title":"MadDiff.jl","text":"","category":"section"},{"location":"#MadDiff.ForwardResult","page":"Home","title":"MadDiff.ForwardResult","text":"ForwardResult{VT}\n\nFields\n\ndx::VT: Primal variable sensitivities\ndλ::VT: Constraint dual sensitivities\ndzl::VT: Lower bound dual sensitivities\ndzu::VT: Upper bound dual sensitivities\n\n\n\n\n\n","category":"type"},{"location":"#MadDiff.MadDiffConfig","page":"Home","title":"MadDiff.MadDiffConfig","text":"MadDiffConfig(; kwargs...)\n\nConfiguration options for MadDiffSolver.\n\nKeyword Arguments\n\nkkt_system::Type: KKT system type for sensitivity analysis. Defaults to re-using the solver's KKT system.\nkkt_options::Dict: Options passed to KKT system constructor.\nlinear_solver::Type: Custom linear solver type for KKT system.\nlinear_solver_options::Dict: Options passed to linear solver constructor.\nreuse_kkt::Bool: Whether to reuse the solver's KKT system (default: true). Ignored if kkt_system, kkt_options,                     linear_solver, or linear_solver_options are provided.\nregularization::Symbol: Regularization strategy (:solver, :none, or :inertia. default: :solver).\ninertia_shift_step::Float64: Step size for inertia-based regularization (default: 1.0e-6).\ninertia_max_corrections::Int: Maximum number of inertia correction attempts (default: 50).\nwarn_condensed_kkt::Bool: Whether to warn when using condensed KKT systems (default: false).\n\n\n\n\n\n","category":"type"},{"location":"#MadDiff.MadDiffSolver","page":"Home","title":"MadDiff.MadDiffSolver","text":"MadDiffSolver(solver; config=MadDiffConfig(), param_pullback=nothing, n_params=0)\n\nCreate a sensitivity solver from a solved MadNLP solver.\n\nArguments\n\nsolver: A solved MadNLP.AbstractMadNLPSolver\nconfig: Optional MadDiffConfig controlling KKT reuse and regularization.           If reuse_kkt=true and no custom KKT options are provided, we           reuse the solver's KKT system; otherwise we build a new one.\nparam_pullback: Optional callback to compute parameter gradients in reverse mode.              Signature: param_pullback(out, adj_x, adj_λ, adj_zl, adj_zu, sens) -> out.              The callback should fill out with ∂L/∂p for each parameter.\nn_params: Number of parameters (required if param_pullback is provided).             Used to pre-allocate the gradient buffer.\n\nExample\n\nsolver = MadNLPSolver(nlp)\n\n# Forward mode:\nsens = MadDiffSolver(solver)\nfwd_result = forward_differentiate!(sens; Dxp_L, Dp_g)\n\n# Reverse mode:\nsens = MadDiffSolver(solver; param_pullback, n_params)\nrev_result = reverse_differentiate!(sens; dL_dx)\n\n\n\n\n\n","category":"type"},{"location":"#MadDiff.ReverseResult","page":"Home","title":"MadDiff.ReverseResult","text":"ReverseResult{VT, GT}\n\nFields\n\nadj_x::VT: Primal adjoint vector (length n_x)\nadj_λ::VT: Dual adjoint vector (length n_con)\nadj_zl::VT: Lower bound dual adjoint (length n_lb)\nadj_zu::VT: Upper bound dual adjoint (length n_ub)\ngrad_p::GT: Parameter gradient vector (if param_pullback callback was set on solver, nothing otherwise)\n\n\n\n\n\n","category":"type"},{"location":"#MadDiff.forward_differentiate!-Tuple{MadDiffSolver}","page":"Home","title":"MadDiff.forward_differentiate!","text":"forward_differentiate!(sens::MadDiffSolver; kwargs...) -> ForwardResult\n\nCompute forward sensitivities (JVP) for a parameter perturbation.\n\nAt least one of the parameter-direction inputs must be provided. Input vectors may be mutated during computation.\n\nKeyword Arguments\n\nDxp_L: Parameter-Lagrangian cross derivative times Δp: (∂²L/∂x∂p) * Δp (length = n_x)\nDp_g: Parameter-constraint LHS Jacobian times Δp: (∂g/∂p) * Δp (length = n_con)\nDp_l: Variable lower bound perturbation times Δp: (∂l/∂p) * Δp (length = n_lb)\nDp_u: Variable upper bound perturbation times Δp: (∂u/∂p) * Δp (length = n_ub)\nDp_lcon: Constraint lower bound perturbation times Δp: (∂lcon/∂p) * Δp (length = n_con)\nDp_ucon: Constraint upper bound perturbation times Δp: (∂ucon/∂p) * Δp (length = n_con)\n\nNotes\n\nFor equality constraints (lcon[i] == ucon[i]), provide the same perturbation for both Dplcon[i] and Dpucon[i]. The implementation uses (Dplcon + Dpucon)/2 for equality constraints. For entries where lcon or ucon is ±Inf, the corresponding Dplcon/Dpucon value is ignored. Dp_lcon and Dp_ucon must not alias each other (pass copies if needed). To obtain a full Jacobian w.r.t. parameters, call this once per parameter direction.\n\nReturns\n\nForwardResult containing dx, dλ, dzl, dzu sensitivity vectors\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.forward_differentiate!-Tuple{MadNLP.AbstractMadNLPSolver}","page":"Home","title":"MadDiff.forward_differentiate!","text":"forward_differentiate!(solver; kwargs...) -> ForwardResult\n\nConvenience function for one-shot forward sensitivity computation. For multiple perturbations, use MadDiffSolver.\n\nArguments\n\nsolver: A solved MadNLP.AbstractMadNLPSolver\nkwargs...: Passed to forward_differentiate! and MadDiffConfig\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.make_param_pullback-Tuple{}","page":"Home","title":"MadDiff.make_param_pullback","text":"make_param_pullback(; Dxp_L=nothing, Dp_g=nothing, Dp_lcon=nothing, Dp_ucon=nothing, Dp_l=nothing, Dp_u=nothing)\n\nCreate a param_pullback callback from parameter derivative matrices.\n\nThe returned callback computes grad_p = ∂L/∂p using:\n\ngrad_p = -Dxp_L' * adj_x - Dp_g' * adj_λ + Dp_con' * adj_λ - Dp_l' * adj_zl + Dp_u' * adj_zu\n\nArguments\n\nDxp_L: ∂²L/∂x∂p - cross derivative of Lagrangian w.r.t. x and p (nx × nparams)\nDp_g: ∂g/∂p - derivative of constraint function w.r.t. p (ncon × nparams)\nDp_lcon: ∂lcon/∂p - derivative of constraint lower bounds (ncon × nparams)\nDp_ucon: ∂ucon/∂p - derivative of constraint upper bounds (ncon × nparams)\nDp_l: ∂l/∂p - derivative of variable lower bounds (nlb × nparams)\nDp_u: ∂u/∂p - derivative of variable upper bounds (nub × nparams)\n\nEquality Constraint Handling\n\nFor equality constraints (lcon == ucon), the contributions from Dp_lcon and Dp_ucon are scaled by 0.5 to handle AD through problem construction naturally giving equal values for both.\n\nExample\n\n# Parameter p affects constraint RHS: g(x) >= p\nparam_pullback = make_param_pullback(Dp_lcon=Dp_lcon)\nsens = MadDiffSolver(solver; param_pullback, n_params)\nresult = reverse_differentiate!(sens; dL_dx)\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.reset_sensitivity_cache!-Tuple{MadDiffSolver}","page":"Home","title":"MadDiff.reset_sensitivity_cache!","text":"reset_sensitivity_cache!(sens::MadDiffSolver) -> MadDiffSolver\n\nClear cached forward/reverse work buffers and refresh the sensitivity KKT factorization. Use after re-solving or modifying solver so that sensitivities reflect the latest solution/problem.\n\nIf not called, sensitivities will be incorrect!\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.reverse_differentiate!-Tuple{MadDiffSolver}","page":"Home","title":"MadDiff.reverse_differentiate!","text":"reverse_differentiate!(sens::MadDiffSolver; dL_dx=nothing, dL_dλ=nothing, dL_dzl=nothing, dL_dzu=nothing) -> ReverseResult\n\nCompute reverse sensitivities (VJP) given loss gradients.\n\nAt least one of dL_dx, dL_dλ, dL_dzl, dL_dzu must be provided. Input vectors may be mutated during computation.\n\nIf a param_pullback callback was provided when constructing the MadDiffSolver, it will be called to compute parameter gradients.\n\nArguments\n\ndL_dx: Gradient of loss with respect to primal variables (length n_x).\ndL_dλ: Gradient of loss with respect to constraint duals (length n_con).\ndL_dzl: Gradient of loss with respect to lower bound duals (length n_lb).\ndL_dzu: Gradient of loss with respect to upper bound duals (length n_ub).\n\nReturns\n\nReverseResult containing:\nadj_x: Primal adjoint vector\nadj_λ: Constraint dual adjoint vector\nadj_zl: Lower bound dual adjoint\nadj_zu: Upper bound dual adjoint\ngrad_p: Parameter gradient vector (if param_pullback was set; nothing otherwise)\n\n\n\n\n\n","category":"method"},{"location":"#MadDiff.reverse_differentiate!-Tuple{MadNLP.AbstractMadNLPSolver}","page":"Home","title":"MadDiff.reverse_differentiate!","text":"reverse_differentiate!(solver; dL_dx=nothing, dL_dλ=nothing, dL_dzl=nothing, dL_dzu=nothing, kwargs...) -> ReverseResult\n\nConvenience function for one-shot reverse sensitivity computation. For multiple gradient computations, use the MadDiffSolver API. Input vectors may be mutated during computation.\n\n\n\n\n\n","category":"method"}]
}
