var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#Jacobian","page":"API Reference","title":"Jacobian","text":"","category":"section"},{"location":"api/#Jacobian-Transpose","page":"API Reference","title":"Jacobian Transpose","text":"","category":"section"},{"location":"api/#Jacobian-Vector-Product","page":"API Reference","title":"Jacobian-Vector Product","text":"","category":"section"},{"location":"api/#Vector-Jacobian-Product","page":"API Reference","title":"Vector-Jacobian Product","text":"","category":"section"},{"location":"api/#MadDiff.diff_optimizer","page":"API Reference","title":"MadDiff.diff_optimizer","text":"MadDiff.diff_optimizer(optimizer_constructor; kwargs...)\n\nWrap an optimizer constructor (e.g. MadNLP.Optimizer) for implicit differentiation with MadDiff.\n\nIntended to be used with JuMP: model = JuMP.Model(MadDiff.diff_optimizer(MadNLP.Optimizer)).\n\n\n\n\n\n","category":"function"},{"location":"api/#MadDiff.MadDiffSolver","page":"API Reference","title":"MadDiff.MadDiffSolver","text":"MadDiffSolver(solver::MadNLP.AbstractMadNLPSolver; config::MadDiffConfig = MadDiffConfig())\n\nCreate a MadDiffSolver from a solved MadNLP.AbstractMadNLPSolver.\n\n\n\n\n\n","category":"type"},{"location":"api/#MadDiff.MadDiffConfig","page":"API Reference","title":"MadDiff.MadDiffConfig","text":"MadDiffConfig\n\nOptions struct for MadDiff. Except for skip_kkt_refactorization, if any options are provided,     MadDiff will create its own KKT system rather than re-using the solver's.\n\nFields\n\nkkt_system::Type: The MadNLP.AbstractKKTSystem to use for implicit differentiation. Example: MadNLP.SparseUnreducedKKTSystem\nkkt_options::Dict: The kwargs to pass to MadNLP.create_kkt_system.\nlinear_solver::Type: The MadNLP.AbstractLinearSolver to use for implicit differentation. Example: MadNLP.MumpsSolver\nlinear_solver_options::Any: The opts to pass to the constructor of linear_solver.\nskip_kkt_refactorization::Bool: If set to true, MadDiff will not refactorize the KKT system before differentiation. Default is false.\n\n\n\n\n\n","category":"type"},{"location":"api/#MadDiff.reset_sensitivity_cache!","page":"API Reference","title":"MadDiff.reset_sensitivity_cache!","text":"reset_sensitivity_cache!(sens::MadDiffSolver)\n\nClear the differentiation caches. Must be called upon changes to the underlying MadNLP.AbstractMadNLPSolver.\n\n\n\n\n\n","category":"function"},{"location":"api/#MadDiff.jacobian!","page":"API Reference","title":"MadDiff.jacobian!","text":"jacobian!(sens::MadDiffSolver)\n\nCompute the Jacobian of the optimal solution with respect to the parameters using forward implicit differentiation.\n\nReturns a JacobianResult containing Jacobian blocks.\n\n\n\n\n\n","category":"function"},{"location":"api/#MadDiff.JacobianResult","page":"API Reference","title":"MadDiff.JacobianResult","text":"JacobianResult{MT,VT}\n\nContainer for the Jacobian of the optimal solution with respect to parameters.\n\nFields are Jacobian blocks with columns corresponding to parameter directions:\n\ndx: ∂x/∂p\ndy: ∂y/∂p\ndzl: ∂zl/∂p\ndzu: ∂zu/∂p\ndobj: ∂obj/∂p (objective gradient w.r.t. parameters)\n\nReturned by jacobian!.\n\n\n\n\n\n","category":"type"},{"location":"api/#MadDiff.jacobian_transpose!","page":"API Reference","title":"MadDiff.jacobian_transpose!","text":"jacobian_transpose!(sens::MadDiffSolver)\n\nCompute the transpose of the Jacobian of the optimal solution with respect to parameters using reverse implicit differentiation.\n\nReturns a JacobianTransposeResult containing Jacobian transpose blocks.\n\n\n\n\n\n","category":"function"},{"location":"api/#MadDiff.JacobianTransposeResult","page":"API Reference","title":"MadDiff.JacobianTransposeResult","text":"JacobianTransposeResult{MT,VT}\n\nContainer for the transpose of the Jacobian of the optimal solution with respect to parameters.\n\nFields represent Jacobian-transpose blocks (rows corresponding to parameters):\n\ndx: (∂x/∂p)ᵀ\ndy: (∂y/∂p)ᵀ\ndzl: (∂zl/∂p)ᵀ\ndzu: (∂zu/∂p)ᵀ\ndobj: (∂obj/∂p)ᵀ\n\nReturned by jacobian_transpose!.\n\n\n\n\n\n","category":"type"},{"location":"api/#MadDiff.jacobian_vector_product!","page":"API Reference","title":"MadDiff.jacobian_vector_product!","text":"jacobian_vector_product!(sens::MadDiffSolver, Δp::AbstractVector)\n\nCompute sensitivities of the optimal solution to a parameter perturbation Δp by evaluating the Jacobian–vector product (JVP) of the KKT system via forward implicit differentiation.\n\nReturns a JVPResult with solution sensitivities dx, dy, dzl, dzu.\n\n\n\n\n\n","category":"function"},{"location":"api/#MadDiff.JVPResult","page":"API Reference","title":"MadDiff.JVPResult","text":"JVPResult{VT,T}\n\nContainer for the result of a Jacobian–vector product (JVP), for the sensitivity of the optimal solution with respect to parameters.\n\nFields store directional sensitivities for a parameter perturbation Δp:\n\ndx: direction for primal variables x\ndy: direction for constraint multipliers y\ndzl: direction for lower-bound multipliers\ndzu: direction for upper-bound multipliers\ndobj: directional derivative of the objective value along Δp\n\nReturned by jacobian_vector_product!.\n\n\n\n\n\n","category":"type"},{"location":"api/#MadDiff.vector_jacobian_product!","page":"API Reference","title":"MadDiff.vector_jacobian_product!","text":"vector_jacobian_product!(sens::MadDiffSolver; dL_dx, dL_dy, dL_dzl, dL_dzu, dobj)\n\nCompute the vector–Jacobian product (VJP) needed to backpropagate a scalar loss through the optimal solution with respect to the parameters, using reverse implicit differentiation.\n\nKeyword arguments provide the loss sensitivities with respect to the primal/dual solution components (dL_dx, dL_dy, dL_dzl, dL_dzu). A \"shortcut\" objective contribution is also accepted under dobj. All are optional, but at least one must be provided.\n\nReturns a VJPResult containing the parameter gradient grad_p.\n\n\n\n\n\n","category":"function"},{"location":"api/#MadDiff.VJPResult","page":"API Reference","title":"MadDiff.VJPResult","text":"VJPResult{VT,GT}\n\nContainer for the result of a vector–Jacobian product (VJP), for backpropagating a scalar loss through the optimal solution.\n\nFields:\n\ndx, dy, dzl, dzu: adjoints for the solution components (the solved reverse sensitivities)\ngrad_p: gradient of the loss with respect to parameters\n\nReturned by vector_jacobian_product!.\n\n\n\n\n\n","category":"type"},{"location":"#MadDiff.jl","page":"Home","title":"MadDiff.jl","text":"(Image: Dev) (Image: Build Status) (Image: Coverage)\n\nMadDiff implements forward and reverse mode implicit differentiation for MadSuite solvers. MadDiff leverages MadNLP's modular KKT and linear solver infrastructure, supporting LP, QP, and NLP using KKT systems from MadNLP, MadIPM, MadNCL, and HybridKKT.\n\nMadDiff is a work-in-progress and requires installing forks of several dependencies. Proceed with caution and verify correctness before use.","category":"section"},{"location":"#NLPModels-interface","page":"Home","title":"NLPModels interface","text":"The NLPModels interface requires that your AbstractNLPModel implementation includes the ParametricNLPModels API. Currently, this is automated only for the case when using MadNLP through JuMP, but support for ExaModels, ADNLPModels, and NLPModelsJuMP is planned.\n\nnlp = ...  # must implement ParametricNLPModels API\nsolver = MadNLP.MadNLPSolver(nlp)\nsolution = MadNLP.solve!(solver)\n\ndiff = MadDiff.MadDiffSolver(solver)\n\ndL_dx, dL_dy, dL_dzl, dL_dzu = ...  # loss sensitivity vectors\nrev = MadDiff.vector_jacobian_product!(diff; dL_dx, dL_dy, dL_dzl, dL_dzu)\nrev.grad_p  # gradient of the loss with respect to the parameters","category":"section"},{"location":"#JuMP-interface","page":"Home","title":"JuMP interface","text":"MadDiff aims to be a drop-in replacement for DiffOpt with MadNLP. Simply switch DiffOpt.diff_optimizer(MadNLP.Optimizer) for MadDiff.diff_optimizer(MadNLP.Optimizer) and enjoy the speedup!\n\nusing JuMP, DiffOpt\nusing MadDiff, MadNLP\n\nmodel = Model(MadDiff.diff_optimizer(MadNLP.Optimizer))  # use MadDiff.diff_optimizer\n@variable(model, x)\n@variable(model, p in MOI.Parameter(1.0))\n@constraint(model, x >= 2p)\n@objective(model, Min, x^2)\noptimize!(model)\n\nDiffOpt.empty_input_sensitivities!(model)\nMOI.set(model, DiffOpt.ForwardConstraintSet(), ParameterRef(p), MOI.Parameter(1.0))\nDiffOpt.forward_differentiate!(model)\ndx = MOI.get(model, DiffOpt.ForwardVariablePrimal(), x)\n\nDiffOpt.empty_input_sensitivities!(model)\nMOI.set(model, DiffOpt.ReverseVariablePrimal(), x, 1.0)\nDiffOpt.reverse_differentiate!(model)\ndp = MOI.get(model, DiffOpt.ReverseConstraintSet(), ParameterRef(p)).value","category":"section"}]
}
